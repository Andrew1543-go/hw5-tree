import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from collections import namedtuple
from scipy import optimize

List = namedtuple('List', ('value', 'x', 'y'))
Node = namedtuple('Node', ('feature', 'value', 'impurity', 'left', 'right',))

class BaseDecisionTree:
    def __init__(self, x, y, max_depth=np.inf, min_impurity_split=1e-6):
        self.x = np.atleast_2d(x)
        self.y = np.atleast_1d(y)
        self.max_depth = max_depth
        
        self.min_impurity_split = min_impurity_split
        
        self.features = x.shape[1]
        
        self.root = self.build_tree(self.x, self.y)
    
    
    def build_tree(self, x, y, depth=1):
        if depth > self.max_depth or self.criteria(y) < self.min_impurity_split:
            return List(self.list_value(y), x, y)
        
        feature, value, impurity = self.find_best_split(x, y)
        
        left_xy, right_xy = self.partition(x, y, feature, value)
        left = self.build_tree(*left_xy, depth=depth + 1)
        right = self.build_tree(*right_xy, depth=depth + 1)
        
        return Node(feature, value, impurity, left, right)
    
    def list_value(self, y):
        raise NotImplementedError
    
    def partition(self, x, y, feature, value):
        i = x[:, feature] >= value
        j = np.logical_not(i)
        return (x[j], y[j]), (x[i], y[i])
    
    def _impurity_partition(self, value, feature, x, y):
        (_, left), (_, right) = self.partition(x, y, feature, value)
        return self.impurity(left, right)
    
    def find_best_split(self, x, y):
        best_feature, best_value, best_impurity = 0, x[0,0], np.inf
        for feature in range(self.features):
            if x.shape[0] > 2:
                x_interval = np.sort(x[:,feature])
                res = optimize.minimize_scalar(
                    self._impurity_partition, 
                    args=(feature, x, y),
                    bounds=(x_interval[1], x_interval[-1]),
                    method='Bounded',
                )
                assert res.success
                value = res.x
                impurity = res.fun
            else:
                value = np.max(x[:,feature])
                impurity = self._impurity_partition(value, feature, x, y)
            if impurity < best_impurity:
                best_feature, best_value, best_impurity = feature, value, impurity
        return best_feature, best_value, best_impurity
    

    def impurity(self, left, right):
        h_lef = self.criteria(left)
        h_rig = self.criteria(right)
        return (left.size * h_lef + right.size * h_rig) / (left.size + right.size)
    
    def criteria(self, y):
        raise NotImplementedError
        
    def predict(self, x):
        x = np.atleast_2d(x)
        y = np.empty(x.shape[0], dtype=self.y.dtype)
        for i, row in enumerate(x):
            node = self.root
            while not isinstance(node, List
                                 ):
                if row[node.feature] >= node.value:
                    node = node.right
                else:
                    node = node.left
            y[i] = node.value
        return y
        
                
class DecisionTreeRegressor(BaseDecisionTree):
    def __init__(self, x, y, *args, random_state=None, **kwargs):
        y = np.asarray(y, dtype=float)
        self.random_state = np.random.RandomState(random_state)
        self.classes = np.unique(y)
        super().__init__(x, y, *args, **kwargs)
        
    def list_value(self, y):
        class_counts = np.sum(y == self.classes.reshape(-1,1), axis=1)
        m = np.max(class_counts)
        most_common = self.classes[class_counts == m]
        if most_common.size == 1:
            return most_common[0]
        return self.random_state.choice(most_common)
    
    def criteria(self, y):
        return np.std(y)

class RandomForestRegressor:
    def __init__(self, x, y, n_trees, depth=15, random_seed=15):
        np.random.seed(random_seed)
        
        self.x, self.y, self.depth = x, y, depth
        self.trees = [self.create_tree() for i in range(n_trees)]
        

    def create_tree(self):
        tree_x, xx, tree_y, yy = train_test_split(self.x, self.y, test_size=0.3, random_state=45)
        return DecisionTreeRegressor(tree_x, tree_y, max_depth=self.depth)
        
    def predict(self, x):
        return np.mean([t.predict(x) for t in self.trees], axis=0)
        
        
        
        
        
        
file = pd.read_csv(r'C:\Users\79852\Downloads\sdss_redshift.csv')
xfile = file.iloc[:, :5]
y = file.iloc[:, 5]

X_tr, X_t, y_tr, y_t = train_test_split(xfile, y, train_size=0.5, random_state=42)

forest = RandomForestRegressor(X_tr, y_tr, 17)

y_train = forest.predict(X_tr)
y_test = forest.predict(X_t)


plt.scatter(y_train, y_tr)
plt.scatter(y_test, y_t, color='green')             
